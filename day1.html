<!DOCTYPE html>
<!-- saved from url=(0045)https://mbzuai-cl.github.io/2023/programday1/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!--<base href=".">--><base href=".">
    <link rel="shortcut icon" type="image/png" href="https://mbzuai-cl.github.io/2023/assets/favicon.png">
    <link rel="stylesheet" type="text/css" media="all" href="./MLLM2024_day1_files/main.css">
    <meta name="description" content="MBZUAI Machine Learning for Large Models Workshop 2024">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>MBZUAI-Berkeley Joint Workshop on Emerging Directions of Machine Learning </title>
</head>

<body>

    <div class="banner">
        <img src="./MLLM2024_files/banner.jpeg" alt="MBZUAI Machine Learning for Large Models 2024 Banner">
        <div class="top-left">
            <span class="title2">MBZUAI-Berkeley Joint Workshop on</span>
            <br><br> <span class="title1" style="font-size: 38px;">Emerging Directions of Machine Learning</span> 
            <!-- <br><br>
            <span class="year">Empowering Sustainable Futures</span> -->
        </div>
        <div class="bottom-right">
            Mar 12, 2025 <br> MBZUAI, Abu Dhabi
        </div>
    </div>

    <table class="navigation">
        <tbody><tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href="index.html">Home</a>
            </td>
            <td class="navigation">
                <a title="Speakers List" href="speakerlist.html">Speakers List</a> 
            </td>
            <td class="navigation">
                <a title="Conference Program" href="day1.html">Program</a> 
            </td>
            <!-- <td class="navigation">
                <a title="Conference Program" href="https://mbzuai.ac.ae/">MBZUAI</a> 
            </td> -->
            <!-- <td class="navigation">
                <a title="Conference Program Day 2" href="day2.html">Program Day 2</a> 
            </td> -->
        </tr>
    </tbody></table>

   

    <h2>Program (March 12, Wednesday, Excecutive Theatre)</h2>

    <table id="Eric Xing">
        <tbody><tr class="speaker">
            <td class="date" rowspan="3">
                9:00 am
            </td>
            <td class="title-special">
                Welcome coffee
            </td>
        </tr>
        <!-- <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/professor-eric-xing/"><b>Eric Xing</b></a> (MBZUAI)
            </td>
        </tr> -->
    </tbody></table>

    <table id="Michael I. Jordan">
        <tbody><tr>
            <td class="date" rowspan="3">
                9:30 am
            </td>
            <td class="title">
                Gradient Equilibrium in Online Learning: Theory and Applications
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://people.eecs.berkeley.edu/~jordan/"><b>Michael I. Jordan</b></a> (UC Berkeley)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We present a new perspective on online learning that we refer to as gradient equilibrium: a sequence of iterates achieves gradient equilibrium if the average of gradients of losses along the sequence converges to zero. In general, this condition is not implied by, nor implies, sublinear regret. It turns out that gradient equilibrium is achievable by standard online learning methods such as gradient descent and mirror descent with constant step sizes (rather than decaying step sizes, as is usually required for no regret). Further, as we show through examples, gradient equilibrium translates into an interpretable and meaningful property in online prediction problems spanning regression, classification, quantile estimation, and others. Notably, we show that the gradient equilibrium framework can be used to develop a debiasing scheme for black-box predictions under arbitrary distribution shift, based on simple post hoc online descent updates. We also show that post hoc gradient updates can be used to calibrate predicted quantiles under distribution shift, and that the framework leads to unbiased Elo scores for pairwise preference prediction.  [with Anastasios Angelopoulos and Ryan Tibshirani].
        </td></tr>
    </tbody></table>
    
     <table id="Alireza Fallah">
        <tbody><tr>
            <td class="date" rowspan="3">
                10:00am
            </td>
            <td class="title">
                Fair Allocation in Dynamic Mechanism Design
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://afallah.lids.mit.edu/"><b>Alireza Fallah</b></a> (UC Berkeley)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We consider a dynamic mechanism design problem where an auctioneer sells an indivisible good to groups of buyers in every round, for a total of T rounds. The auctioneer aims to maximize their discounted overall revenue while adhering to a fairness constraint that guarantees a minimum average allocation for each group. We begin by studying the static case (T=1) and establish that the optimal mechanism involves two types of subsidization: one that increases the overall probability of allocation to all buyers, and another that favors the groups which otherwise have a lower probability of winning the item. We then extend our results to the dynamic case by characterizing a set of recursive functions that determine the optimal allocation and payments in each round. Notably, our results establish that in the dynamic case, the seller, on the one hand, commits to a participation bonus to incentivize truth-telling, and on the other hand, charges an entry fee for every round. Moreover, the optimal allocation once more involves subsidization, which its extent depends on the difference in future utilities for both the seller and buyers when allocating the item to one group versus the others. Finally, we present an approximation scheme to solve the recursive equations and determine an approximately optimal and fair allocation efficiently. Based on joint work with Annie Ulichney and Michael I. Jordan.
        </td></tr>
    </tbody></table>

   

     <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                10:30am
            </td>
            <td class="title-special">
                Coffee Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>

 <table id="Ian Waudby-Smith">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:00am
            </td>
            <td class="title">
                Election audits via anytime-valid inference
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://ianws.com/"><b>Ian Waudby-Smith</b></a> (UC Berkeley)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Accurately determining the outcome of an election is a complex task with many potential sources of error, ranging from software glitches in voting machines to procedural lapses to outright fraud. Risk-limiting audits (RLA) are statistically principled "incremental" hand counts that provide statistical assurance that reported outcomes accurately reflect the validly cast votes. We present a suite of tools for conducting RLAs using confidence sequences -- sequences of confidence sets which uniformly capture an electoral parameter of interest from the start of an audit to the point of an exhaustive recount with high probability. Adopting the SHANGRLA framework, we design nonnegative martingales which yield computationally and statistically efficient confidence sequences and RLAs for a wide variety of election types.
        </td></tr>
    </tbody></table>

    <table id="Mingming Gong">
        <tbody><tr>
            <td class="date" rowspan="3">
                11:30am
            </td>
            <td class="title">
                On the Identifiability of ODEs/SDEs for Causal Inference
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/mingming-gong/"><b>Mingming Gong</b></a> (MBZUAI & Unimelb)
            </td>
        </tr>
        <tr>
            <td class="abstract">
            ODEs/SDEs have recently gained significant attention in machine learning and causal inference. However, the theoretical aspects, for example, identifiability and asymptotic properties of statistical estimation are still obscure. In this presentation, I will present our recent results on identifiability of linear ODE/SDEs from observational data. These identifiability conditions are crucial in causal inference using linear ODE/SDEs as they enable the identification of the post-intervention distributions from its observational distribution. 
        </td></tr>
    </tbody></table>
   

   <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                12:00pm
            </td>
            <td class="title-special">
                Lunch
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>

    <table id="Martin Takac">
        <tbody><tr>
            <td class="date" rowspan="3">
                2:00pm
            </td>
            <td class="title">
                Graph Neural Networks for Materials Discovery
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/martin-takac/"><b>Martin Takac</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Machine learning methods—particularly graph neural networks (GNNs)—have emerged as powerful tools for accelerating the search for novel catalytic materials. By representing each material as a graph where atoms are nodes and bonds are edges, GNNs can capture local atomic interactions while still scaling to realistic systems. However, designing a GNN architecture that both assimilates global context and respects essential physical symmetries is nontrivial.
                In this talk, we provide an overview of GNNs for materials discovery and explain how deeper message-passing architectures can improve expressiveness yet risk “over-smoothing”—a phenomenon where atom-level embeddings become indistinguishable. We then highlight PaiNN as one promising approach that ensures rotational and translational equivariance. By incorporating these symmetries directly into the model, PaiNN preserves physically meaningful geometric information and yields more robust and interpretable predictions.
                Finally, we focus on predicting the Density of States (DOS), a property that reveals the distribution of electronic states and is crucial for identifying active sites in catalytic reactions. Accurate DOS predictions help pinpoint where electrons can be exchanged during catalysis, guiding experimental efforts toward more efficient and sustainable catalysts. Through concrete examples, we illustrate how GNN-based DOS modeling can bridge the gap between quantum-mechanical theory and large-scale materials discovery.
        </td></tr>
    </tbody></table>


    <table id="Salem Lahlou">
        <tbody><tr>
            <td class="date" rowspan="3">
                2:30pm
            </td>
            <td class="title">
                GFlowNets: An Introduction and Recent Advances
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/salem-lahlou/"><b>Salem Lahlou</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Generative Flow Networks offer a framework for sampling from reward-proportional distributions in combinatorial and continuous spaces. By leveraging flow conservation principles, GFNs enable diverse exploration where traditional methods like MCMC struggle. This talk introduces the theoretical foundations of GFNs and highlights their practical applications in molecular design, protein structure prediction, and Bayesian network discovery. Special emphasis will be placed on recent advances in applying GFNs to improve the systematic exploration capabilities of large language models.
        </td></tr>
    </tbody></table>

    

    <table id="Maxim Panov">
        <tbody><tr>
            <td class="date" rowspan="3">
                3:00pm
            </td>
            <td class="title">
                TBA
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/maxim-panov/"><b>Maxim Panov</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBA
        </td></tr>
    </tbody></table>

    <table>
        <tbody><tr>
            <td class="date" rowspan="2">
                3:30pm
            </td>
            <td class="title-special">
                Coffee Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </tbody></table>

    <table id="Nils Lukas">
        <tbody><tr>
            <td class="date" rowspan="3">
                4:00pm
            </td>
            <td class="title">
                Leveraging Optimization for Adaptive Attacks against Content 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://mbzuai.ac.ae/study/faculty/nils-lukas/"><b>Nils Lukas</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Large Language Models (LLMs) can be misused to spread online spam and misinformation. Content watermarking deters misuse by hiding a message in model-generated outputs, enabling their detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content’s quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate the robustness of LLM watermarking as an objective function and propose preference-based optimization to tune adaptive attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks substantially outperform non-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks optimized against a few known watermarks remain highly effective when tested against other unseen watermarks and (iii) optimization-based attacks are practical and require less than seven GPU hours. Our findings underscore the need to test robustness against adaptive attackers.
        </td></tr>
    </tbody></table>
    <table id="panel discussion">
        <tbody><tr>
            <td class="date" rowspan="3">
                4:30pm-5:30pm
            </td>
            <td class="title">
                Panel Discussion
            </td>
        </tr>
        <tr>
            <td class="speaker">
                TBA
            </td>
        </tr>
    </tbody></table>

    
</body></html>